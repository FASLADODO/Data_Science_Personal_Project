# Neural Image Caption Generation with Visual Attention

## Objective

he purpose of this project is to build an image captioning predictor. Given an image, the algorithm should predict what is the appropriate caption correspond to the picture. This project follows the step-by-step method from TensorFlow tutorial on image captioning. You can find the corresponding tutorial by TensorFlow [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb#scrollTo=Dx_fvbVgRPGQ).

To do this, the concept of encoder-decoder will be applied. Convolutional Neural Networks is applied as the encoder, while Recurrent Neural Networks is applied as the decoder.

The dataset used for this project was taken from MS COCO 2017 dataset, which contains hundreds of thousand of images with their corresponding captions. To speed up the training process, only 20000 images will be used as training and validation set.

Below is the example of the output generated by this project.

<p align="center">
  <img width="700" height="700" src="https://github.com/marcellusruben/Data_Science_Personal_Project/blob/master/Image_Captioning_with_Visual_Attention/caption.png">
</p>


## File

There is only one file in this project, which is the Jupyter Notebook which contains the step-by-step method applied in this project.
