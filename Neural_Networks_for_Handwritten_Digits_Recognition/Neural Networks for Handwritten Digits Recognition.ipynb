{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Neural Networks Algorithm for Handwritten Digits Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to build neural networks algorithm to recognize the handwritten digits. The data for this project was taken from Prof. Andrew Ng's machine learning class on Coursera, which is a file with M extension.\n",
    "\n",
    "The data consists of 5000 handwritten digits, each digits is represented by 20 x 20 pixel grayscale image. Let's visualize the data in order to know more about how the handwritten digits looks like. Before that, we need to import necessary libraries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "handwritingData = loadmat('Data.mat')\n",
    "\n",
    "xData = handwritingData['X']\n",
    "yData  =handwritingData['y']\n",
    "\n",
    "# Since the original data map digits \"0\" to \"10\", we need to map it back to \"0\"\n",
    "for i in range(len(yData)):\n",
    "    \n",
    "    if yData[i] == 10:\n",
    "        \n",
    "        yData[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our feature sets and our response set. Next, we need to define a function to show the image of the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeTheNumber (xData, matrixRow):\n",
    "    \n",
    "    for row in range (len(matrixRow)):\n",
    "        \n",
    "        matrixData = xData[matrixRow[row],:]\n",
    "        matrixData = matrixData.reshape(20,20)\n",
    "        \n",
    "        plt.subplot(3,5,row+1)\n",
    "        plt.imshow(matrixData.transpose(), cmap='gray_r')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create a variable containing a list of the number of rows in the feature set in which the number we want to visualize. Then, pass it to the function that we have just define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 15 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = [1, 600, 1000, 1620, 2000, 2640, 3102, 3700, 4600, 2344, 3211, 20, 4300, 2222, 1234]\n",
    "visualizeTheNumber(xData,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it. The visualization of the handwritten digits from 0 to 9. Since we have 10 different kinds of digits, then it means that we will have 10 different classifications as the output. As the input itself, since each of the 20 x 20 pixels digit image is unrolled into 400 rows of 1D vector, then we will have 400 features as the input layer. Meanwhile we will have 25 units for the hidden layer, which brings us to the neural networks schema as follows:\n",
    "\n",
    "<img src=\"schema.png\" width=\"700\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the schema above, now we need to define the variables needed to describe the input layers and hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = np.ones((len(xData),1)) #for bias value.\n",
    "xDataNew = np.column_stack((intercept,xData)) #xData + bias\n",
    "\n",
    "inputLayerSize = len(xDataNew[0])\n",
    "hiddenLayerSize = 25\n",
    "noOfLabels = 10 # numbe rof output\n",
    "lambdaRegularization = 1 #for regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Random Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to delve into neural networks algorithm, first we need to randomly initialize the weight of our parameeters $\\theta$. Normally in other algorithm, we can set our initial $\\theta$ = 0, but for neural networks, setting our initial $\\theta$ to 0 would lead to a non-optimal learning process because we basically give each parameter the same weight. So, it is important to initialize the weight for symmetry breaking. \n",
    "\n",
    "So let's create a function to initialize the $\\theta$ value to small number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeight(outputVal, inputVal):\n",
    "    \n",
    "    epsilon = 0.12 #It can be adjusted according to your need\n",
    "    thetaWeight = np.zeros((outputVal, inputVal))\n",
    "    thetaWeight = np.random.rand(outputVal, inputVal) * 2*epsilon-epsilon\n",
    "    \n",
    "    return thetaWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define our $\\theta$ by calling our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08832092,  0.01912664, -0.04255007,  0.08755889,  0.02458611])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta1 = initializeWeight(hiddenLayerSize,inputLayerSize) #Theta1 should have the size of num units in hidden layer x num of units in input layer. \n",
    "Theta2 = initializeWeight(noOfLabels,hiddenLayerSize+1) # Theta2 should have the size of num units in output layer x num of units in hidden layer.\n",
    "\n",
    "Theta1[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's check the shape of our $\\theta$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 401), (10, 26))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Theta1), np.shape(Theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good and as expected so far. Now with this initial $\\theta$ value, we can start to define our neural networks algorithm from forward propagation to back propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step for neural networks algorithm is the feedforward propagation. Basically the main point of feedforward propagation is to compute the hypothesis $h_\\theta(x^{(i)})$. The algorithm for feedforward propagation by looking at the neural networks model is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{(2)} = \\theta^{(1)}*a^{(1)}$$\n",
    "$$a^{(2)} = g(z^{(2)})$$\n",
    "\n",
    "and then we add $a_0^{(2)} = 0$ for bias value in the hidden layer.\n",
    "\n",
    "$$z^{(3)} = \\theta^{(2)}*a^{(2)}$$\n",
    "$$h_\\theta(x) = a^{(3)} = g(z^{(3)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $z^{(2)}$ is basically the matrix multiplication of the feature value $x$ with the transpose of our initial value of $\\theta_1$. Meanwhile $a^{(2)}$ is our feature $x$ in the hidden layer after calculating the sigmoid function of $z^{(2)}$. The same applies for $z^{(3)}$ and $a^{(3)}$. In the end our, hypothesis function $h_\\theta(x)$ is equal to $a^{(3)}$.\n",
    "\n",
    "The formula to calculate the sigmoid function is given by:\n",
    "$$\\frac{1}{(1+exp(z))}$$\n",
    "\n",
    "So first, let's define a function to compute this sigmoid function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidFunction(x):\n",
    "    \n",
    "    hypothesisFunct = 1/(1+np.exp(-x))\n",
    "    \n",
    "    return hypothesisFunct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply the feedforward algorithm together with regularized cost function and back propagation below. So, let's jump to computing the regularized cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the hypothesis function $h_\\theta(x)$ , then we can start to compute the regularized cost function $J$ with the following equation:\n",
    "$$J(\\theta) =\n",
    "\\frac{1}{m}\\sum_{i=1}^m\n",
    "\\sum_{k=1}^K \n",
    "\\left[ -y^{(i)}_k \\log((h_{\\theta}(x^{(i)})_k)- (1 -y^{(i)}_k) \\log(1-(h_{\\theta}(x^{(i)}))_k)  \\right]\n",
    "\\\\ \\qquad \n",
    "+\\frac{\\lambda}{2m} \\left[\\sum_{j=1}^{25} \n",
    "\\sum_{k=1}^{400} {\\left( \\theta_{j,k}^{(1)}\\right)^2}+\n",
    "\\sum_{j=1}^{10} \n",
    "\\sum_{k=1}^{25} {\\left( \\theta_{j,k}^{(2)}\\right)^2}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $m$ is the number of training sets, which is 5000 and $K$ is the number of possible outputs, which in our case is 10. Meanwhile, $y$ is the response variable and $\\lambda$ is the regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for us to compute the back propagation algorithm. The main focus of back propagation is to compute the error term $\\delta_j^{(l)}$ that measures how much error each input $j$ in layer $l$ contributes for the error of the output. Below is the formula for the back propagation algorithm by looking at the neural networks model.\n",
    "\n",
    "$$\\delta^{(3)} = (h_\\theta(x)) - y$$\n",
    "$$\\delta^{(2)} = (\\theta^{(2)})^T * \\delta^{(3)} . * g'(z^{(2)})$$\n",
    "\n",
    "where $g'(z^{(2)})$ is the gradient of the sigmoid function and can be computed by the formula:\n",
    "$$g'(z)=\\frac{d}{dz}g(z) = g(z)(1-g(z))$$\n",
    "\n",
    "Next, let's quickly define a function to compute the gradient of sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradientFunction(x):\n",
    "    \n",
    "    hypothesisFunct = np.multiply((1/(1+np.exp(-x))),(1-(1/(1+np.exp(-x)))))\n",
    "    \n",
    "    return hypothesisFunct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can calculate the capital delta, which is the gradient, as: $$\\Delta^{(l)} = \\delta^{(l+1)} * (a^{(l)})^T$$\n",
    "\n",
    "And then sum all of the $\\Delta^{(l)}$.\n",
    "\n",
    "Finally, we can get the partial derivative of our cost function $J$ with respect to $\\theta$ with the following formula.\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta}J(\\theta) = \\frac{1}{m} \\Delta + \\lambda\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after writing all of the tedious equations, now we can implement it in the code. Let's define a function to calculate the cost function $J$ and $\\frac{\\partial}{\\partial \\theta}J(\\theta)$ with feedforward propagation, regularized cost function, and finally back propagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostFunctionNeuralNetworks(theta1, theta2, x, y, labels, experiments, lambdaReg):\n",
    "    \n",
    "    # Convert yData into a matrix -------------------------------------------------------\n",
    "    \n",
    "    yMatrix = np.zeros((len(y),labels))\n",
    "    I = np.identity(labels)\n",
    "    for i in range (len(y)):\n",
    "        yMatrix[i,:] = I[y[i],:]\n",
    "    \n",
    "    # Forward Propagation Algorithm-------------------------------------------------------\n",
    "    \n",
    "    z2 = np.matmul(x,theta1.transpose())\n",
    "    a2 = sigmoidFunction(z2)\n",
    "    \n",
    "    a2Bias = np.ones((len(a2),1))\n",
    "    a2PlusBias = np.column_stack((a2Bias,a2))\n",
    "    \n",
    "    z3 = np.matmul(a2PlusBias,theta2.transpose())\n",
    "    a3 = sigmoidFunction(z3)\n",
    "    \n",
    "    #Compute Unregularized Cost -----------------------------------------------\n",
    "    \n",
    "    frontEq = np.multiply(-yMatrix,np.log(a3))\n",
    "    tailEq = np.multiply(1-yMatrix,np.log(1-a3))\n",
    "    concatEq = frontEq-tailEq\n",
    "    \n",
    "    rowSummation = np.sum(concatEq,axis=1)\n",
    "    colSummation = np.sum(rowSummation)\n",
    "    \n",
    "    J = (1/experiments)*colSummation\n",
    "    \n",
    "    # Compute Regularized Cost -------------------------------------------------\n",
    "    \n",
    "    # Convert first column to 0 because bias coefficient shouldn't be regularized\n",
    "    \n",
    "    theta1[:,0] = 0\n",
    "    theta2[:,0] = 0\n",
    "    \n",
    "    theta1Reg = theta1**2\n",
    "    theta2Reg = theta2**2\n",
    "    \n",
    "    rowSummationTheta1 = np.sum(theta1Reg,axis=1)\n",
    "    colSummationTheta1 = np.sum(rowSummationTheta1)\n",
    "    \n",
    "    rowSummationTheta2 = np.sum(theta2Reg,axis=1)\n",
    "    colSummationTheta2 = np.sum(rowSummationTheta2)\n",
    "    \n",
    "    J = J+((lambdaReg/(2*experiments)))*(colSummationTheta1+colSummationTheta2)\n",
    "    \n",
    "    # Backpropagation Algorithm ----------------------------------------------------------\n",
    "    \n",
    "    d3 = a3-yMatrix\n",
    "    theta2WithoutBias = theta2[:,1:]\n",
    "    a2Derivative = sigmoidGradientFunction(z2)\n",
    "    d2 = np.multiply(np.matmul(d3,theta2WithoutBias),a2Derivative)\n",
    "    \n",
    "    delta1 = np.matmul(d2.transpose(),x)\n",
    "    delta2 = np.matmul(d3.transpose(),a2PlusBias)\n",
    "    \n",
    "    theta1Gradient = (1/experiments)*delta1\n",
    "    theta2Gradient = (1/experiments)*delta2\n",
    "    \n",
    "    scaledTheta1 = (lambdaReg/experiments)*theta1Gradient\n",
    "    scaledTheta2 = (lambdaReg/experiments)*theta2Gradient\n",
    "    \n",
    "    theta1Gradient = theta1Gradient+scaledTheta1\n",
    "    theta2Gradient = theta2Gradient+scaledTheta2\n",
    "    \n",
    "    return J, theta1Gradient, theta2Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, if we take a look at the final partial derivative term of $\\frac{\\partial}{\\partial \\theta}J(\\theta)$, it is an important term in order to obtain an updated value of our initial $\\theta$ value with gradient descent optimization. The algorithm to update $\\theta$ values with gradient descent optimization is as follows:\n",
    "\n",
    "$$\\theta_j:=\\theta_j-\\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha$ is the learning rate that we need to define in advance. By iterating the computation of the cost function using gradient descent optimization and doing the feedforward and back propagation again and again, the $\\theta$ values will be updated and the cost function will be minimized.\n",
    "\n",
    "So, let's create a function to perform gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentNeuralNetworks(x, y, theta1, theta2, alpha, noOfExperiments, iterations, labels, lambdaReg):\n",
    "    \n",
    "    JHistory = np.zeros((iterations,1))\n",
    "    \n",
    "    for i in range (iterations):\n",
    "        \n",
    "        JHistory[i], theta1Grad, theta2Grad = computeCostFunctionNeuralNetworks(theta1, theta2, x, y, labels, noOfExperiments, lambdaReg)\n",
    "        \n",
    "\n",
    "        theta1 = theta1 - (alpha*theta1Grad)\n",
    "        theta2 = theta2 - (alpha*theta2Grad)\n",
    "        \n",
    "    return JHistory, theta1, theta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we also need to define some necessary variables in advance, such as how many iterations we want to run the gradient descent optimization and how big is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 3\n",
    "numOfIterations = 2000\n",
    "noOfTraining = len(yData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can run the neural netwroks algorithm now by calling the function we define above and pass all the necessary variables as the input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "J, finTheta1, finTheta2 = gradientDescentNeuralNetworks(xDataNew, yData, Theta1, Theta2, alpha, noOfTraining, numOfIterations, noOfLabels, lambdaRegularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59317059])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the final cost function $J$ in the last iterations. Now we need to calculate the percentage of the accuracy of this neural networks model by comparing the result to our response variable $y$. In order to do that, we need to calculate the sigmoid function for our final values of $\\theta_1$ and then use the result to compute sigmoid function of $\\theta_2$, just like the feedforward propagation. \n",
    "\n",
    "The result of this computation is 4000 x 10 matrix, each column represent the probability of each row having particular label of output (digits 0-9). Finally, for each row, we need to return the column in which it has the highest probability value, so at the end we have 4000 rows of 1D vector, just like our response variable $y$. In order to do this, let's define a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictResultNeuralNetworks(theta1, theta2, x):\n",
    "    \n",
    "    probabilityList = np.zeros((len(x),1))\n",
    "    \n",
    "    h1 = sigmoidFunction(np.matmul(x,theta1.transpose()))\n",
    "    h1Bias = np.ones((len(x),1))\n",
    "    h1PlusBias = np.column_stack((h1Bias,h1))\n",
    "    h2 = sigmoidFunction(np.matmul(h1PlusBias,theta2.transpose()))\n",
    "    \n",
    "    probabilityList = np.argmax(h2, axis=1)\n",
    "    \n",
    "    return probabilityList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we call the function that we just define above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "accProbability = predictResultNeuralNetworks(finTheta1, finTheta2, xDataNew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to compare our 4000 rows of 1D vector with the response variable $y$ and check, how many rows that they match with each other. After that, we can calculate the final accuracy percentage of the neural networks model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.8"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accNew = np.asarray(accProbability.reshape((len(yData),1)))\n",
    "    \n",
    "modelAccuracy = np.mean((accNew == yData))*100\n",
    "modelAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it! The final percentage of the neural networks algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
