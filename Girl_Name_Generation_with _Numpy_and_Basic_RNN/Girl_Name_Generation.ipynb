{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Girl Name Generation with Numpy and Basic RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine there is a young couple expecting the born of their first baby child. They soon find out that the baby will be a girl but they are not sure yet what to name her. The purpose of this project is to generate girl name recommendation with the help of character level basic and simple RNN architecture. \n",
    "\n",
    "Because the low level API using numpy will be applied, then the very simple one layer RNN architecture is applied to avoid the overly complex algorithm. A text file contains about 3000 girl names is provided and can be seen from this GitHub repo https://github.com/dominictarr/random-name/blob/master/first-names.txt.\n",
    "\n",
    "Before we jump into the RNN architecture, let's import all necessary libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we need to do is to preprocess the text data. First, we need to read the text file. Then, we need to convert all of the words into lowercase to maintain the consistency. Thirdly, we need to create a list of unique characters found in the text data. \n",
    "\n",
    "Let's do all of the steps explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('girl_name.txt', 'r').read()\n",
    "data= data.lower() #Transform all of characters into lowercase\n",
    "chars = list(set(data)) # Create set of unique characters\n",
    "data_size, vocab_size = len(data), len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 unique characters in your text data\n"
     ]
    }
   ],
   "source": [
    "print('There are '+str(vocab_size)+' unique characters in your text data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the ouput above, we have 29 unique characters from the text data. Let's see what those unique characters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ouput above, we can see that we have three unusual characters, which are \"\\n\", \" \", and \"-\". \" \" and \"-\" are special characters that we normally find in certain people's name, like for example Mary-Anne or van Domersmack for example. Meanwhile \"\\n\" means an end of a word, or similar as token EOS (End-of-Sentence). \n",
    "\n",
    "Next, we need to create a dictionary that will be beneficial to map a character to its corresponding index and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = { key: value for value,key in enumerate(chars) }\n",
    "idx_to_char = { value:key for value,key in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to build the simple Recurrent Neural Networks model. Below is the figure about how the architecture of RNN model in this project looks like:\n",
    "\n",
    "<img src=\"rnn.png\" style=\"width:600 ; height:220px;\">\n",
    "\n",
    "In this project, there will be only one simple RNN layer, as shown in figure above. The step on how to compute the simple RNN model is as follows:\n",
    "\n",
    "- At the first time step, the input character $x^{\\langle 1 \\rangle}$ and the hidden state $a^{\\langle 0 \\rangle}$ will be set to 0.\n",
    "- Next, we need to run one step of forward propagation in between layers in order to get the hidden state at the next time step, $a^{\\langle 1 \\rangle}$ and the character output in that time step, $\\hat{y}^{\\langle 1 \\rangle}$.\n",
    "\n",
    "The equation to compute hidden state, activation function, and the prediction can be seen as follows:\n",
    "\n",
    "Hidden state:\n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t+1 \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)$$\n",
    "Activation:\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y $$\n",
    "Prediction:\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })$$\n",
    "\n",
    "Before we build a function to do all of these steps, first let's define a function to compute the softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define a function to do all of the steps defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_cell(parameters, char_to_idx, seed):\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # Creating a zero vector x that can be used as the one-hot vector \n",
    "    # representing the first character. \n",
    "    x = np.zeros((vocab_size,1))\n",
    "    \n",
    "    # Initialize hidden state at previous time step as zeros.\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "   \n",
    "    # Create an empty list of indices.\n",
    "    indices = []\n",
    "    \n",
    "    # idx is the index of the one-hot vector x that is set to 1. In this case, initialize idx to -1.\n",
    "    idx = -1 \n",
    "    \n",
    "    counter = 0\n",
    "    newline_character = char_to_idx['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50): # maximize the number of character in each word to 50.\n",
    "        \n",
    "        # Forward propagate x.\n",
    "        a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev)+b)\n",
    "        z = np.dot(Wya,a)+by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        # Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        # so that it will not always generate the same character given a previous character.\n",
    "        idx = np.random.choice(vocab_size, p = y.ravel())\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Overwrite the input x with one that corresponds to the sampled index `idx`.\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # Update the hidden state to current time steps.\n",
    "        a_prev = a\n",
    "        \n",
    "        counter +=1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_idx['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to initialize all of the weights and bias parameters that we need to update in each epochs during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \n",
    "    Wax = np.random.randn(n_a, n_x)*0.01  # input to hidden\n",
    "    Waa = np.random.randn(n_a, n_a)*0.01  # hidden to hidden\n",
    "    Wya = np.random.randn(n_y, n_a)*0.01  # hidden to output\n",
    "    b = np.zeros((n_a, 1)) # hidden bias\n",
    "    by = np.zeros((n_y, 1)) # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's time to forward propagate the RNN model. The forward propagation steps in this model is exactly the same as the one that already defined in `RNN_cell` function. First, the input vector $x$ and input hidden state $a$ will be multiplied by their corresponding weights. Then the bias term is added and finally, the tanh activation function is applied to add non-linearity to the model. Finally, the softmax activation function is used to predict the output character based on one time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars \n",
    "    \n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function above, we performed a feed forward propagation for one RNN cell. Hence, let's define a function that will do those operations in every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, parameters, vocab_size = 29):\n",
    "    \n",
    "    # Initialize x, a and y_hat as empty dictionaries\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    \n",
    "    a[-1] = np.copy(a0)\n",
    "    \n",
    "    # initialize loss to 0\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(X)): #iterate every time step\n",
    "        \n",
    "        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n",
    "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        \n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
    "        \n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "    \n",
    "    # Store the result in a cache which will be useful for backpropagation\n",
    "    cache = (y_hat, a, x)\n",
    "        \n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the functions to do forward propagation has just defined. Next, let's define a function for backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everybody can agree that the most difficult algorithm to model in a deep learning architecture is its backpropagation algorithm and RNN is not an exception. It requires a lot of derivations and for some people it is not that intuitive. Luckily, there is a closed form solution for basic RNN with softmax activation function such that what we need to do is simply apply this mathematical formulation to the algorithm.\n",
    "\n",
    "Let's define a function to compute the backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] \n",
    "    daraw = (1 - a * a) * da \n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function defined above, we only applied the backpropagation algorithm in one time step. Let's define a function such that we run the backpropagation in every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \n",
    "    # Initialize gradients as an empty dictionary\n",
    "    gradients = {}\n",
    "    \n",
    "    # Retrieve from cache and parameters\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "\n",
    "    # Backpropagate through time\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "\n",
    "    \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Update, Gradient Clipping, and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying backpropagation algorithm in every time step, now it is time to use the gradients of weights and bias parameters to update their corresponding variable. The application of this updating is very straightforward, in which we subtract the weight with the learning rate and the gradients. Below is the formula for parameters updating:\n",
    "\n",
    "$$ W^{[t]} = W^{[t]} - \\alpha \\text{ } dW^{[t]}$$\n",
    "$$ b^{[t]} = b^{[t]} - \\alpha \\text{ } db^{[t]}$$\n",
    "\n",
    "Let's define a function to do this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've basically defined all the necessary functions for optimization process. However, it is a common theory that one of the biggest pet peeve in using simple RNN architecture is that sometimes it has a problem with its gradient, either a problem with vanishing gradient or exploding gradient.\n",
    "\n",
    "One of the technique to deal with exploding gradient in simple RNN architecture is clipping gradient. With clipping gradient, the gradient result that is too large or too low will be supressed into a pre-defined maximum value or minimum value, hence the exploding gradient problem can be avoided. Let's define a function for gradient clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientClip(gradients, max_value):\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        \n",
    "        np.clip(gradient, -max_value, max_value, out = gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define a function to run a full loop of optimization process. This function will wrap all of the steps defined above, from forward propagation, back propagation, parameter updates, and the application of clipping gradient. Let's define this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \n",
    "    # Forward propagate through time\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # Backpropagate through time\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip gradients between -5 (min) and 5 (max)\n",
    "    gradients = gradientClip(gradients, 5)\n",
    "    \n",
    "    # Update parameter\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the wrap up function for optimization process has already defined. But, the defined function will only run for one single epochs. Hence, we need to build a model that will run the optimization function depending on the number of epochs that we defined in advance.\n",
    "\n",
    "Before we build the model to wrap up all of the process, let's define a print statement function so that we can see the recommendation of girl's name generated at the last epochs or in the last training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    print ('%s' % (txt, ), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can build the final wrap up model, that will run the optimization process depending the number of epochs that we specified in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, idx_to_char, char_to_idx, epochs = 150001, n_a = 50, girl_names = 8, vocab_size = 29):\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Build list of all girl names.\n",
    "    with open(\"girl_name.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # Shuffle list of all girl names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(epochs):\n",
    "       \n",
    "        # Set the index `idx`\n",
    "        idx = j%len(examples)\n",
    "        \n",
    "        # Set the input X\n",
    "        single_example = examples[idx]\n",
    "        single_example_chars = [c for c in single_example]\n",
    "        \n",
    "        single_example_idx =  [char_to_idx[c] for c in single_example_chars]\n",
    "       \n",
    "        X = [None] + single_example_idx\n",
    "        \n",
    "        # Set the labels Y\n",
    "        idx_newline = char_to_idx['\\n']\n",
    "        Y = single_example_idx+[idx_newline]\n",
    "        \n",
    "        \n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        \n",
    "        loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "\n",
    "        # Print output\n",
    "        if j % (epochs-1) == 0:\n",
    "            \n",
    "            # The number of girl names to print\n",
    "            for name in range(girl_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = RNN_cell(parameters, char_to_idx, 0)\n",
    "                print_sample(sampled_indices, idx_to_char) \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Girl Name Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to generate the girl name recommendation. In order to do this, all that we need to do is just simply call the `model` function defined above and passing the text data as well as the mapping from character to index and vice versa that we already defined in the very beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aodgiayzjeyurkitmqnr-hbdcl\n",
      "Ym qeavca sp\n",
      "Drbndkwfqj-hvzvbhvntt-gkyxj-imxffplcblttzii\n",
      "Vptkx\n",
      "Ah--ndpxue-yzhkbrjun guqpqopomshkidg gi-dedhbsgaoc\n",
      " bjrp\n",
      "Tgq\n",
      "I\n",
      "\n",
      "\n",
      "Leri\n",
      "Shilly\n",
      "Shaysenannne\n",
      "Chirelia\n",
      "Dido\n",
      "Kathy\n",
      "Kaunan\n",
      "Lisle\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = model(data, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have different girl names recommendations! As we can see from results above, at epochs 1, the algorithm generates a set of word recommendations that doesn't make sense at all. However, with the help of character level basic RNN architecture and gradient descent optimization, the algorithm learns the patterns of various girl names based on the input data and generates new girl names for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
