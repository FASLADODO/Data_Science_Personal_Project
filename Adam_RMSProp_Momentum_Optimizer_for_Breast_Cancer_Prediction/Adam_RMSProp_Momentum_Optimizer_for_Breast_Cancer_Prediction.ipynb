{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam, RMSProp, and Momentum Mini-Batch Gradient Descent Optimizer for Breast Cancer Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to apply Adam, RMSProp, and Momentum mini-batch gradient descent for the breast cancer prediction. Then, the F1 score of the test set using these optimizers will be compared with one another.\n",
    "\n",
    "The data used in this project was taken from the Wisconsin Breast Cancer datasets, available at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29. Overall, there are 569 examples of tumor with different size of radius, texture, concavity, smoothness, and compactness available.\n",
    "\n",
    "First, let's import all of the necessary libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import math\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use mini-batch gradient descent, the first step that we need to do is to partition the training set as well as test set into several mini-batches. In this project, each mini-batch consists of 64 training and test examples.\n",
    "\n",
    "However, since each mini-batch consists of 64 elements, then it is natural that the last mini-batch will have elements less than 64. We can compute the number of elements in the last mini-batch with the following formula:\n",
    "\n",
    "$$m-mini_\\_batch_\\_size \\times \\lfloor \\frac{m}{mini\\_batch\\_size}\\rfloor$$\n",
    "\n",
    "where $m$ is the total examples of training set, $mini_\\_batch_\\_size$ is the size of each mini-batch, which is 64, and $\\lfloor \\rfloor$ is the sign to rounding down float number to closest integer.\n",
    "\n",
    "Let's implement the mini batch size in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size, seed =0):\n",
    "    \n",
    "    np.random.seed(seed)            \n",
    "    m = X.shape[1]                  \n",
    "    mini_batches = []\n",
    "        \n",
    "    # Shuffling the training set and test set\n",
    "    permutation = list(np.random.permutation(m))\n",
    "\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "  \n",
    "    # Partition to mini batches\n",
    "    num_minibatches = math.floor(m/mini_batch_size) # number of generated mini-batches\n",
    "    \n",
    "    for k in range(0, num_minibatches):\n",
    "        \n",
    "        mini_batch_X = shuffled_X[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "  \n",
    "    # Handling the end tail of mini batches which have elements < 64\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "       \n",
    "        mini_batch_X = shuffled_X[:, 0:m - mini_batch_size*math.floor(m/mini_batch_size)]\n",
    "        mini_batch_Y = shuffled_Y[:, 0:m - mini_batch_size*math.floor(m/mini_batch_size)]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above consists of two steps: \n",
    "- First, the training set and the corresponding response value $Y$ is shuffled randomly to generate random mini-batches.\n",
    "- Second, the shuffled training set and the corresponding response value $Y$ is partitioned into mini-batches of 64 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first optimizer that will be used in this project is Momentum optimizer. One of the biggest challenge in applying pure mini-batch gradient descent is that the parameters update will occur only after seeing just a subset of training set (i.e each mini-batch), hence there will be oscillations in the process of reaching the optimum value.\n",
    "\n",
    "In order to overcome this, let's apply the momentum mini-batch gradient descent. The advantage of applying momentum is that it reduces the oscillations that will typically be found in the mini-batch gradient descent during parameter updates and thus, speeds up the learning rate.\n",
    "\n",
    "Equations for momentum optimizer:\n",
    "\n",
    "$$v_{d_w} = \\beta*v_{d_w} + (1-\\beta) dW$$\n",
    "$$v_{d_b} = \\beta*v_{d_b} + (1-\\beta) db$$\n",
    "$$W = W - \\alpha v_{d_W}, b = b - \\alpha v_{d_b}$$\n",
    "\n",
    "where $v_{d_w}$ is the direction of previous gradient for the weight $W$, $v_{d_b}$ is the direction of previous gradient for the bias term $b$, and $\\beta$ is momentum parameter.\n",
    "\n",
    "Let's implement mathematical equations above in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeVelocity_momentum(parameters):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    \n",
    "    for l in range(L):\n",
    "    \n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters['W' + str(l+1)].shape))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters['b' + str(l+1)].shape))\n",
    "        \n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " def updateParameters_with_momentum(parameters, grads, v, beta, learningRate):\n",
    "   \n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = beta*v[\"dW\" + str(l+1)] + ((1-beta)*grads['dW' + str(l+1)])\n",
    "        v[\"db\" + str(l+1)] = beta*v[\"db\" + str(l+1)] + ((1-beta)*grads['db' + str(l+1)])\n",
    "    \n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learningRate*grads['dW' + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learningRate*grads['db' + str(l+1)]\n",
    "     \n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the equation above, we can see that if we set $\\beta$ = 0, then we have the same parameters updating as if we are not applying any Momentum. The bigger the $\\beta$, the smoother the updates will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp Optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind RMSProp is basically the same as momentum mini-batch gradient descent, in which we try to dampen the oscillations of the path in each parameter updates of mini-batch gradient descent. The difference between RMSProp and Momentum is the term to update the weight $W$ and bias term $b$. In RMSProp, the root mean squared technique is applied with the formula:\n",
    "\n",
    "$$S_{d_w} = \\beta*S_{d_w} + (1-\\beta) dW^2$$\n",
    "$$S_{d_b} = \\beta*S_{d_b} + (1-\\beta) db^2$$\n",
    "$$W = W - \\alpha \\frac{dW}{\\sqrt{S_{d_w}}}$$\n",
    "$$b = b - \\alpha \\frac{db}{\\sqrt{S_{d_b}}}$$\n",
    "\n",
    "Now let's apply the formula above in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeVelocity_RMSProp(parameters):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    \n",
    "    for l in range(L):\n",
    "    \n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters['W' + str(l+1)].shape))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters['b' + str(l+1)].shape))\n",
    "        \n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def updateParameters_with_RMSProp(parameters, grads, s, beta, learningRate, epsilon):\n",
    "   \n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "\n",
    "        s[\"dW\" + str(l+1)] = beta*s[\"dW\" + str(l+1)] + ((1-beta)*grads['dW' + str(l+1)]**2)\n",
    "        s[\"db\" + str(l+1)] = beta*s[\"db\" + str(l+1)] + ((1-beta)*grads['db' + str(l+1)]**2)\n",
    "    \n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learningRate*grads['dW' + str(l+1)]/np.sqrt(s[\"dW\" + str(l+1)]+epsilon) \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learningRate*grads['db' + str(l+1)]/np.sqrt(s[\"db\" + str(l+1)]+epsilon)\n",
    "     \n",
    "        \n",
    "    return parameters, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam optimizer is the optimizer that combines both of the intuition from Momentum and RMSProp mini-batch gradient descent. The formula for Adam optimizer is as follows:\n",
    "\n",
    "$$ v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } $$\n",
    "$$v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t}$$\n",
    "$$s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2$$\n",
    "$$s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t}$$\n",
    "$$W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}$$\n",
    "\n",
    "where:\n",
    "- $t$ is the number of steps taken.\n",
    "- $\\beta_1$ and $\\beta_2$ are the hyperparameters controlling the exponentially weighted averages. \n",
    "- $\\varepsilon$ is the small number to avoid the division by zero.\n",
    "\n",
    "As we can see from the mathematical equation above, adam optimizer uses the combination of formula that are already applied in Momentum and RMSProp optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeAdam(parameters) :\n",
    " \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "   \n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters_with_adam(parameters, grads, v, s, t, learningRate, beta1, beta2, epsilon):\n",
    "\n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                        \n",
    "    \n",
    "    \n",
    "    for l in range(L):\n",
    "      \n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+((1-beta1)*grads['dW' + str(l+1)])\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+((1-beta1)*grads['db' + str(l+1)])\n",
    "\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-(beta1)**t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-(beta1)**t)\n",
    "\n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+((1-beta2)*(grads['dW' + str(l+1)])**2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+((1-beta2)*(grads['db' + str(l+1)])**2)\n",
    "      \n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-(beta2)**t)\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-(beta2)**t)\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learningRate*v_corrected[\"dW\" + str(l+1)]/((np.sqrt(s_corrected[\"dW\" + str(l+1)]))+ epsilon))\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learningRate*v_corrected[\"db\" + str(l+1)]/((np.sqrt(s_corrected[\"db\" + str(l+1)]))+ epsilon))\n",
    "    \n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we already define the optimizer functions necessary for the purpose of this project. Next, we need to build a function to wrap-up all of the steps that we defined so far. This function will take all of the steps from deep neural networks and apply the algorithm according to the optimizer of our choose.\n",
    "\n",
    "The algorithm for deep neural networks has already been done in another project called \"Deep Neural Networks with Different Activation Functions for Breast Cancer Prediction\". So in this project, all we need to do is just call the corresponding function from that project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layersDimension, activation, optimizer, learningRate, mini_batch_size, beta, beta1, \n",
    "          beta2,  epsilon, num_epochs):\n",
    "\n",
    "\n",
    "    L = len(layersDimension)         # number of layers in the neural networks\n",
    "    costs = []                       \n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                      \n",
    "    m = X.shape[1]                   # number of training examples\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initializeParameters(layersDimension)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"rmsprop\":\n",
    "        s = initializeVelocity_RMSProp(parameters)\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initializeVelocity_momentum(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initializeAdam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            AL, caches = modelForwardProp(minibatch_X, parameters, activation)\n",
    "\n",
    "            # Compute cost and add to the cost total\n",
    "            cost_total += computeCost_mini_batch(AL, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = modelBackProp(AL, minibatch_Y, caches, activation)\n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"rmsprop\":\n",
    "                parameters, s = updateParameters_with_RMSProp(parameters, grads, s, beta2, learningRate, epsilon)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = updateParameters_with_momentum(parameters, grads, v, beta, learningRate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s = updateParameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learningRate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Print the cost every 1000 epoch\n",
    "        if i % 10000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we already defined all of the function necessary to run mini-batch gradient descent with different optimizers. Next, let's load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "trainX = pd.read_csv('trainX.csv')\n",
    "trainY = pd.read_csv('trainY.csv', names = ['diagnosis'])\n",
    "testX = pd.read_csv('testX.csv')\n",
    "testY = pd.read_csv('testY.csv', names = ['diagnosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the training and test data are already splitted into the proportion of 70%-30%. Each feature of the data has been normalized as well using min-max approach. Hence, the data now is ready to be used for classification study.\n",
    "\n",
    "Let's take a look first at the size of the training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training set: (398, 30)\n",
      "Shape of the test set: (171, 30)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the training set: '+str(trainX.shape))\n",
    "print('Shape of the test set: '+str(testX.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can take a look at the first five rows of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.607175</td>\n",
       "      <td>0.420697</td>\n",
       "      <td>0.595743</td>\n",
       "      <td>0.473595</td>\n",
       "      <td>0.412386</td>\n",
       "      <td>0.255567</td>\n",
       "      <td>0.346532</td>\n",
       "      <td>0.472068</td>\n",
       "      <td>0.263636</td>\n",
       "      <td>0.084035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689790</td>\n",
       "      <td>0.502665</td>\n",
       "      <td>0.679267</td>\n",
       "      <td>0.543846</td>\n",
       "      <td>0.528495</td>\n",
       "      <td>0.279138</td>\n",
       "      <td>0.429073</td>\n",
       "      <td>0.820619</td>\n",
       "      <td>0.237138</td>\n",
       "      <td>0.138463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.494060</td>\n",
       "      <td>0.536016</td>\n",
       "      <td>0.488632</td>\n",
       "      <td>0.341251</td>\n",
       "      <td>0.433059</td>\n",
       "      <td>0.292068</td>\n",
       "      <td>0.394096</td>\n",
       "      <td>0.327883</td>\n",
       "      <td>0.125253</td>\n",
       "      <td>0.183235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360726</td>\n",
       "      <td>0.427772</td>\n",
       "      <td>0.348573</td>\n",
       "      <td>0.205417</td>\n",
       "      <td>0.350855</td>\n",
       "      <td>0.147481</td>\n",
       "      <td>0.223882</td>\n",
       "      <td>0.377663</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.086187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.317052</td>\n",
       "      <td>0.223876</td>\n",
       "      <td>0.303849</td>\n",
       "      <td>0.183245</td>\n",
       "      <td>0.362372</td>\n",
       "      <td>0.163088</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.093439</td>\n",
       "      <td>0.288384</td>\n",
       "      <td>0.244103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281750</td>\n",
       "      <td>0.218017</td>\n",
       "      <td>0.254943</td>\n",
       "      <td>0.144564</td>\n",
       "      <td>0.364723</td>\n",
       "      <td>0.125263</td>\n",
       "      <td>0.096326</td>\n",
       "      <td>0.299107</td>\n",
       "      <td>0.244628</td>\n",
       "      <td>0.149416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.273984</td>\n",
       "      <td>0.395671</td>\n",
       "      <td>0.264184</td>\n",
       "      <td>0.154358</td>\n",
       "      <td>0.314706</td>\n",
       "      <td>0.143028</td>\n",
       "      <td>0.072915</td>\n",
       "      <td>0.142346</td>\n",
       "      <td>0.320202</td>\n",
       "      <td>0.271904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207044</td>\n",
       "      <td>0.305970</td>\n",
       "      <td>0.192390</td>\n",
       "      <td>0.096908</td>\n",
       "      <td>0.149970</td>\n",
       "      <td>0.060628</td>\n",
       "      <td>0.041422</td>\n",
       "      <td>0.164021</td>\n",
       "      <td>0.121033</td>\n",
       "      <td>0.089663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.333617</td>\n",
       "      <td>0.390260</td>\n",
       "      <td>0.317877</td>\n",
       "      <td>0.195080</td>\n",
       "      <td>0.343685</td>\n",
       "      <td>0.153580</td>\n",
       "      <td>0.034255</td>\n",
       "      <td>0.094235</td>\n",
       "      <td>0.230808</td>\n",
       "      <td>0.176706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263252</td>\n",
       "      <td>0.486674</td>\n",
       "      <td>0.238358</td>\n",
       "      <td>0.130333</td>\n",
       "      <td>0.379912</td>\n",
       "      <td>0.120315</td>\n",
       "      <td>0.049768</td>\n",
       "      <td>0.273643</td>\n",
       "      <td>0.130298</td>\n",
       "      <td>0.138594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0     0.607175      0.420697        0.595743   0.473595         0.412386   \n",
       "1     0.494060      0.536016        0.488632   0.341251         0.433059   \n",
       "2     0.317052      0.223876        0.303849   0.183245         0.362372   \n",
       "3     0.273984      0.395671        0.264184   0.154358         0.314706   \n",
       "4     0.333617      0.390260        0.317877   0.195080         0.343685   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0          0.255567        0.346532             0.472068       0.263636   \n",
       "1          0.292068        0.394096             0.327883       0.125253   \n",
       "2          0.163088        0.041050             0.093439       0.288384   \n",
       "3          0.143028        0.072915             0.142346       0.320202   \n",
       "4          0.153580        0.034255             0.094235       0.230808   \n",
       "\n",
       "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                0.084035  ...      0.689790       0.502665         0.679267   \n",
       "1                0.183235  ...      0.360726       0.427772         0.348573   \n",
       "2                0.244103  ...      0.281750       0.218017         0.254943   \n",
       "3                0.271904  ...      0.207044       0.305970         0.192390   \n",
       "4                0.176706  ...      0.263252       0.486674         0.238358   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0    0.543846          0.528495           0.279138         0.429073   \n",
       "1    0.205417          0.350855           0.147481         0.223882   \n",
       "2    0.144564          0.364723           0.125263         0.096326   \n",
       "3    0.096908          0.149970           0.060628         0.041422   \n",
       "4    0.130333          0.379912           0.120315         0.049768   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0              0.820619        0.237138                 0.138463  \n",
       "1              0.377663        0.007491                 0.086187  \n",
       "2              0.299107        0.244628                 0.149416  \n",
       "3              0.164021        0.121033                 0.089663  \n",
       "4              0.273643        0.130298                 0.138594  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And first five rows of response variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis\n",
       "0          1\n",
       "1          1\n",
       "2          0\n",
       "3          0\n",
       "4          0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define all of the constant parameters that we will need in order to run the gradient descent like the learning rate $\\alpha$, the number of iterations, as well as the number of hidden layers and the hidden inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX =trainX.values\n",
    "\n",
    "trainY = trainY.values\n",
    "trainY = trainY.reshape((len(trainY),1))\n",
    "\n",
    "testY = testY.values\n",
    "testY = testY.reshape((len(testY),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerDimension = [30, 5, 3, 1] #4 layers with 2 hidden layers\n",
    "learningRate = 0.0001\n",
    "num_epochs = 100010\n",
    "beta = 0.9\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "activation = 'relu'\n",
    "mini_batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the classification that will be conducted is about the severity of the tumor: whether the tumor is benign ot malignant, then the accuracy metrics would not be the best metrics for this condition. The emphasize should be focused in supressing the amount of false negative in the algorithm because the cost of having miss-classified an actual positive will be huge. \n",
    "\n",
    "Just imagine where there is a patient that has a breast tumor and we make a decision that the tumor is benign when in fact it is malignant. This situation will endanger the patient life and thus, creating a machine learning algorithm that can supress the amount of false negative should be a priority.\n",
    "\n",
    "Because of this, instead of accuracy or Jaccard index, the metrics that will be used for this problem is F1-score, which is the 'average' of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, activation):\n",
    "    \n",
    "    m = X.shape[1] \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = modelForwardProp(X, parameters, activation)\n",
    "\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    truePositive = []\n",
    "    trueNegative = []\n",
    "    falsePositive = []\n",
    "    falseNegative = []\n",
    "    \n",
    "    for i in range (0, probas.shape[1]):\n",
    "        truePositive.append((p[0,i] == 1 and y[0,i] == 1))\n",
    "        trueNegative.append(p[0,i] == 0 and y[0,i] == 0)\n",
    "        falsePositive.append(p[0,i] == 1 and y[0,i] == 0)\n",
    "        falseNegative.append(p[0,i] == 0 and y[0,i] == 1)\n",
    "    \n",
    "    epsilon = 10e-8\n",
    "    recall = truePositive.count(True)/(truePositive.count(True)+falseNegative.count(True)+epsilon)\n",
    "    precision = truePositive.count(True)/(truePositive.count(True)+falsePositive.count(True)+epsilon)\n",
    "    \n",
    "    F1_score = 2*(precision*recall/(precision+recall+epsilon))\n",
    "        \n",
    "    return F1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Optimizer Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first optimizer that will be implemented here is the Momentum optimizer. First, the functions that already defined in the previous sections will be called. Then, the metrics F1 score can be used to quantify how well the model is predicting the training set and test set using Momentum optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.695905\n",
      "Cost after epoch 10000: 0.654073\n",
      "Cost after epoch 20000: 0.643471\n",
      "Cost after epoch 30000: 0.534853\n",
      "Cost after epoch 40000: 0.169943\n",
      "Cost after epoch 50000: 0.103632\n",
      "Cost after epoch 60000: 0.087595\n",
      "Cost after epoch 70000: 0.077848\n",
      "Cost after epoch 80000: 0.071317\n",
      "Cost after epoch 90000: 0.066766\n",
      "Cost after epoch 100000: 0.063319\n"
     ]
    }
   ],
   "source": [
    "parameters = model(trainX.T, trainY.T, layerDimension, activation, 'momentum', learningRate, mini_batch_size, \n",
    "                   beta, beta1, beta2,  epsilon, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict the F1 score from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97508891729335"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTrain = predict(trainX.T, trainY.T, parameters, 'relu')\n",
    "predictTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And F1-score from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9701492023279153"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTest = predict(testX.T, testY.T, parameters, 'relu')\n",
    "predictTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the F1 score from training set is 97,5%, which is very good, while the F1 score from the training set is 97%. This means that the Momentum optimizer with seleced constant parameters is able to generalize the test data really well.\n",
    "\n",
    "Next let's see how RMSProp optimizer performs for this particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RMSProp optimizer, the learning rate $\\alpha$ needs to be re-adjusted to the smaller steps because if we run the optimizer with the default learning rate, then the gradient descent would diverge into a nonsense solution. Hence, the smaller learning rate, which is $10^{-5}$, will be used for this optimizer in order to ensure that the gradient descent will be updated in a small step in each epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.695697\n",
      "Cost after epoch 10000: 0.328326\n",
      "Cost after epoch 20000: 0.181345\n",
      "Cost after epoch 30000: 0.115237\n",
      "Cost after epoch 40000: 0.079268\n",
      "Cost after epoch 50000: 0.056441\n",
      "Cost after epoch 60000: 0.052054\n",
      "Cost after epoch 70000: 0.047193\n",
      "Cost after epoch 80000: 0.044094\n",
      "Cost after epoch 90000: 0.040674\n",
      "Cost after epoch 100000: 0.036392\n"
     ]
    }
   ],
   "source": [
    "parameters = model(trainX.T, trainY.T, layerDimension, activation, 'rmsprop', learningRate, mini_batch_size, \n",
    "                   beta, beta1, beta2,  epsilon, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can observe the F1-score of RMSProp optimizer for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9823321047884256"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTrain = predict(trainX.T, trainY.T, parameters, 'relu')\n",
    "predictTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the F1-score from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9411764192149681"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTest = predict(testX.T, testY.T, parameters, 'relu')\n",
    "predictTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, the F1-score from the prediction set is about 98,2%, which is even better than the one with Momentum optimizer. However, the F1-score from the test set is just 94.1%, which means it performs worse than Momentum optimizer. From the differnce between training test and the test set, we can conclude that the model is overfitting the training data. The possible solution for this is by applying regularization methods such as L2 or dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last optimizer that will be investigated is Adam optimizer. The learning rate that will be used for this optimizer is the same as the one used in RMSProp, since if we apply learning rate the same as Momentum, then the gradient descent will diverge.\n",
    "\n",
    "Let's look how this optimizer performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.695902\n",
      "Cost after epoch 10000: 0.332563\n",
      "Cost after epoch 20000: 0.183415\n",
      "Cost after epoch 30000: 0.116519\n",
      "Cost after epoch 40000: 0.079799\n",
      "Cost after epoch 50000: 0.056686\n",
      "Cost after epoch 60000: 0.052223\n",
      "Cost after epoch 70000: 0.047291\n",
      "Cost after epoch 80000: 0.044181\n",
      "Cost after epoch 90000: 0.040723\n",
      "Cost after epoch 100000: 0.036438\n"
     ]
    }
   ],
   "source": [
    "parameters = model(trainX.T, trainY.T, layerDimension, activation, 'adam', learningRate, mini_batch_size, \n",
    "                   beta, beta1, beta2,  epsilon, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see the F1-score generated by the model from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9823321047884256"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTrain = predict(trainX.T, trainY.T, parameters, 'relu')\n",
    "predictTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the F1-score from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9411764192149681"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTest = predict(testX.T, testY.T, parameters, 'relu')\n",
    "predictTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the F1-score result from both training and the test set, we can see that Adam optimizer performs similarly with RMSProp for particular setting of hyperparameters. The conclusion is the same, that there is an overfitting in the training set and we have a variance problem. The solution to remove this problem is by applying regularization algorithm like L2 regularization or dropout in some of the hidden units in the hidden layers of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mini-batch gradient descent with different optimizers has been performed to predict the severity of cancer prediction. Below is the summary of the result.\n",
    "\n",
    "| Optimizer          | Learning Rate | Training | Test      |\n",
    "|-----------------------|-----------|----------|-----------|\n",
    "| Momentum              | $10^{-4}$ | 97.5%    | 97%       |\n",
    "| RMSProp               | $10^{-5}$ | 98.2%    | 94.1%     | \n",
    "| Adam                  | $10^{-5}$ | 98.2%    | 94.1%     | \n",
    "\n",
    "It can be seen from summary above that there is a variance problem in RMSProp and Adam optimizer, meaning that we overfit the training data. The solution for this is to apply regularization method for the parameters in the hidden units in the hidden layers of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
