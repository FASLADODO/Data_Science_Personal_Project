{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks with Different Activation Functions for Breast Cancer Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to apply deep neural networks for the breast cancer prediction. One of the hyperparameter will be fined tuned, which is the type of activation functions between hidden layers. The activation functions that will be tuned in this project are ReLu, sigmoid, and tanh functions and see which activation functions yield to the best prediction of the breast cancer.\n",
    "\n",
    "The data used in this project was taken from the Wisconsin Breast Cancer datasets, available at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29. Overall, there are 569 examples of tumor with different size of radius, texture, concavity, smoothness, and compactness available.\n",
    "\n",
    "First, let's import all of the necessary libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the calculation for the neural networks model, first let's look at the neural networks schema that will be applied in this project. The neural networks will have three layers: two hidden layers and one output layers. The first hidden layer consists of 5 hidden units and the second hidden layer consists of 3 hidden units, while the output layer consists of 1 output unit.\n",
    "\n",
    "Below is the schema of neural networks model.\n",
    "\n",
    "<img src=\"schema_DNN.png\" width=\"700\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization of the Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural networks to learn optimally, initializing the weight randomly is very important. If the weight of each hidden units is asisgned to 0, then the hidden layers would basically useless. Hence, let's initialize the parameters randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeParameters (layerDimension):\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layerDimension)     \n",
    "\n",
    "    for l in range(1, L):\n",
    "       \n",
    "        parameters['W' + str(l)] = np.random.randn(layerDimension[l],layerDimension[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layerDimension[l],1))\n",
    "\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layerDimension[l], layerDimension[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layerDimension[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in the code above, $W$ represents weight in each of the hidden units and $b$ represents the bias. The $W$ is multiplied by 0.01 to make sure that the weight will be a small number to make the optimization process run faster. The relationship between $W$, $b$, and the training variable $X$ can be described as below:\n",
    "\n",
    "$$ W*X+b $$\n",
    "\n",
    "After define the function to define the weight, now let's move forward to forward propagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of forward propagation algorithm is to compute the function $Z$ before the application of activation functions. The formula to compute this linear forward algorithm is:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "where $[l]$ denotes the number of layers and $A$ is the variable in each of the hidden unit in each hidden layer. This means that for $A^{[0]} = X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearForward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W,A)+b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, there is a variable called `cache`. This`cache` will basically store the value of $A$, $W$, and $b$ because all of these values will be used again later in the backpropagation algorithm.\n",
    "\n",
    "Next, we need to compute linear activation forwards, in which we turn the $Z$ value that we got into the result of selected activation functions. In this project, the activation functions that will be used including sigmoid function, ReLu function, and tanh function. The formula of each function can be seen in the following:\n",
    "\n",
    "$$ sigmoid: \\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$$ \n",
    "$$ ReLu: A = RELU(Z) = max(0, Z)$$\n",
    "$$ tanh: \\sigma(Z) = \\sigma(W A + b) = \\frac{e^{(W A + b)}-e^{-(W A + b)}}{e^{(W A + b)}+e^{-(W A + b)}}$$\n",
    "\n",
    "So, let's define a function for each of this activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \n",
    "    A = np.tanh(Z)\n",
    "    cache = Z \n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cache above will store the value of $Z$, in which will be needed again whenever we compute the backpropagationa nd the gradient of each activation function.\n",
    "\n",
    "Now let's build a function that call the function to compute the $Z$ as well as turning $Z$ into new value after the application of activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activationForward(A_prev, W, b, activation):\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "\n",
    "        Z, linear_cache = linearForward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "     \n",
    "    elif activation == \"relu\":\n",
    "\n",
    "        Z, linear_cache = linearForward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        \n",
    "        Z, linear_cache = linearForward(A_prev,W,b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the cache will stor two different types of cache, one for linear cache, which consists of parameter W, A, and b, and the other one is activation cache, which consists parameter Z. These caches will be useful for backpropagation algorithm.\n",
    "\n",
    "Now, the computation for forward propagation is basically done. However, the computation so far is only valid in one layer. If we want to build deep neural networks with many hidden layers, then we need to do the steps above several times depending on the number of layers in the neural networks. Let's define a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelForwardProp(X, parameters,activation):\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                 \n",
    " \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        \n",
    "        A, cache = activationForward(A_prev,parameters[\"W\" + str(l)],parameters[\"b\" + str(l)],activation)\n",
    "        \n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = activationForward(A,parameters[\"W\" + str(L)],parameters[\"b\" + str(L)],'sigmoid')\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compute the cost function for the deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function for deep neural networks is the same as the one in logistic regression. It can be expressed as the following formula:\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(A^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- A^{[L](i)}\\right))$$\n",
    "\n",
    "Now let's define a function to compute the cost based on the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -1/m*np.sum(np.multiply(Y,np.log(AL))+np.multiply(1-Y,np.log(1-AL)))\n",
    "\n",
    "    cost = np.squeeze(cost) \n",
    "    #assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we define the cost function, now finally let's start with backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of backpropagation is to compute the gradient of cost function with respect to each parameters. As the first step, let's define a linear backward step. In forward propagation, the linear step can be defined as:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "In backpropagation, we need to compute the derivative of $W$, $A$, and $b$, which are $dW$, $dA$, and $db$. To do this, we use the following formula:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} =  W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearBackward(dZ, cache):\n",
    "   \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m*np.dot(dZ,A_prev.T)\n",
    "    db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the linear cache that alrady defined in the `linearForward` function, which consists of parameter A, W, and b will be used to compute their derivative based on the formula above.\n",
    "\n",
    "Next, let's compute the derivative of $Z$, which is $dZ$ using the gradient of the selected activation functions with the formula:\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "\n",
    "where $ g'(Z^{[l]})$ is the gradient of the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reluBackward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidBackward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanhBackward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    dZ = dA*(1-np.power(Z,2))\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we use the activation cache that already stored during the calculation of the sigmoid function in the forward propagation. As the result, the derivative of $Z$, $dZ$ can be computed. \n",
    "\n",
    "Next, let's define a function to call the function above and calculate the $dZ$ and then $dW$, $dA$, and $dB$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activationBackward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ = reluBackward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linearBackward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "       \n",
    "        dZ = sigmoidBackward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linearBackward(dZ, linear_cache)\n",
    "     \n",
    "    elif activation == \"tanh\":\n",
    "       \n",
    "        dZ = tanhBackward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linearBackward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we define the steps of back propagation in one layer, then we need to create function that will compute the back propagation depending on the number of layers that we defined, just like the one that already defined in forward propagation.\n",
    "\n",
    "However, we need to calculate the derivative of $A$, $dA$ as the initialization of the back propagation algorithm. This derivative of $dA$ can be computed by the partial derivative of loss function with respect to $A$ at the output layer (or the $Y_{hat}$). The formula of $dA$ is as follows:\n",
    "\n",
    "\n",
    "$$ dA = - \\frac{Y^{[L]}}{A^{[L]}}+\\frac{(1-Y^{[L]})}{(1-A^{[L]})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    " def modelBackProp(AL, Y, caches, activation):   \n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = activationBackward(dAL, current_cache, 'sigmoid')\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = activationBackward(grads[\"dA\"+str(l+1)], current_cache, activation)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp \n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the code above is the value for $dW$, $dA$, and $db$, which we can finally use to update the corresponding variables in each iterations of gradient descent optimization. The formula for updating the parameter in gradient descent can be computed using the following formula:\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate of the gradient descent, the parameter that we need to tune as well. Next, let's define a function to update the parameters based on the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate*grads[\"dW\"+str(l + 1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate*grads[\"db\"+str(l + 1)])\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we already complete all of the functions necessary in order to do the whole loop of forward propagation and backward propagation. Next, let's load the datasets necessary for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "trainX = pd.read_csv('trainX.csv')\n",
    "trainY = pd.read_csv('trainY.csv', names = ['diagnosis'])\n",
    "testX = pd.read_csv('testX.csv')\n",
    "testY = pd.read_csv('testY.csv', names = ['diagnosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the training and test data are already splitted into the proportion of 70%-30%. Each feature of the data has been normalized as well using min-max approach. Hence, the data now is ready to be used for classification study.\n",
    "\n",
    "Let's take a look first at the size of the training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training set: (398, 30)\n",
      "Shape of the test set: (171, 30)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the training set: '+str(trainX.shape))\n",
    "print('Shape of the test set: '+str(testX.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can see the first five rows of training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.607175</td>\n",
       "      <td>0.420697</td>\n",
       "      <td>0.595743</td>\n",
       "      <td>0.473595</td>\n",
       "      <td>0.412386</td>\n",
       "      <td>0.255567</td>\n",
       "      <td>0.346532</td>\n",
       "      <td>0.472068</td>\n",
       "      <td>0.263636</td>\n",
       "      <td>0.084035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689790</td>\n",
       "      <td>0.502665</td>\n",
       "      <td>0.679267</td>\n",
       "      <td>0.543846</td>\n",
       "      <td>0.528495</td>\n",
       "      <td>0.279138</td>\n",
       "      <td>0.429073</td>\n",
       "      <td>0.820619</td>\n",
       "      <td>0.237138</td>\n",
       "      <td>0.138463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.494060</td>\n",
       "      <td>0.536016</td>\n",
       "      <td>0.488632</td>\n",
       "      <td>0.341251</td>\n",
       "      <td>0.433059</td>\n",
       "      <td>0.292068</td>\n",
       "      <td>0.394096</td>\n",
       "      <td>0.327883</td>\n",
       "      <td>0.125253</td>\n",
       "      <td>0.183235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360726</td>\n",
       "      <td>0.427772</td>\n",
       "      <td>0.348573</td>\n",
       "      <td>0.205417</td>\n",
       "      <td>0.350855</td>\n",
       "      <td>0.147481</td>\n",
       "      <td>0.223882</td>\n",
       "      <td>0.377663</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.086187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.317052</td>\n",
       "      <td>0.223876</td>\n",
       "      <td>0.303849</td>\n",
       "      <td>0.183245</td>\n",
       "      <td>0.362372</td>\n",
       "      <td>0.163088</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.093439</td>\n",
       "      <td>0.288384</td>\n",
       "      <td>0.244103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281750</td>\n",
       "      <td>0.218017</td>\n",
       "      <td>0.254943</td>\n",
       "      <td>0.144564</td>\n",
       "      <td>0.364723</td>\n",
       "      <td>0.125263</td>\n",
       "      <td>0.096326</td>\n",
       "      <td>0.299107</td>\n",
       "      <td>0.244628</td>\n",
       "      <td>0.149416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.273984</td>\n",
       "      <td>0.395671</td>\n",
       "      <td>0.264184</td>\n",
       "      <td>0.154358</td>\n",
       "      <td>0.314706</td>\n",
       "      <td>0.143028</td>\n",
       "      <td>0.072915</td>\n",
       "      <td>0.142346</td>\n",
       "      <td>0.320202</td>\n",
       "      <td>0.271904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207044</td>\n",
       "      <td>0.305970</td>\n",
       "      <td>0.192390</td>\n",
       "      <td>0.096908</td>\n",
       "      <td>0.149970</td>\n",
       "      <td>0.060628</td>\n",
       "      <td>0.041422</td>\n",
       "      <td>0.164021</td>\n",
       "      <td>0.121033</td>\n",
       "      <td>0.089663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.333617</td>\n",
       "      <td>0.390260</td>\n",
       "      <td>0.317877</td>\n",
       "      <td>0.195080</td>\n",
       "      <td>0.343685</td>\n",
       "      <td>0.153580</td>\n",
       "      <td>0.034255</td>\n",
       "      <td>0.094235</td>\n",
       "      <td>0.230808</td>\n",
       "      <td>0.176706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263252</td>\n",
       "      <td>0.486674</td>\n",
       "      <td>0.238358</td>\n",
       "      <td>0.130333</td>\n",
       "      <td>0.379912</td>\n",
       "      <td>0.120315</td>\n",
       "      <td>0.049768</td>\n",
       "      <td>0.273643</td>\n",
       "      <td>0.130298</td>\n",
       "      <td>0.138594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0     0.607175      0.420697        0.595743   0.473595         0.412386   \n",
       "1     0.494060      0.536016        0.488632   0.341251         0.433059   \n",
       "2     0.317052      0.223876        0.303849   0.183245         0.362372   \n",
       "3     0.273984      0.395671        0.264184   0.154358         0.314706   \n",
       "4     0.333617      0.390260        0.317877   0.195080         0.343685   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0          0.255567        0.346532             0.472068       0.263636   \n",
       "1          0.292068        0.394096             0.327883       0.125253   \n",
       "2          0.163088        0.041050             0.093439       0.288384   \n",
       "3          0.143028        0.072915             0.142346       0.320202   \n",
       "4          0.153580        0.034255             0.094235       0.230808   \n",
       "\n",
       "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                0.084035  ...      0.689790       0.502665         0.679267   \n",
       "1                0.183235  ...      0.360726       0.427772         0.348573   \n",
       "2                0.244103  ...      0.281750       0.218017         0.254943   \n",
       "3                0.271904  ...      0.207044       0.305970         0.192390   \n",
       "4                0.176706  ...      0.263252       0.486674         0.238358   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0    0.543846          0.528495           0.279138         0.429073   \n",
       "1    0.205417          0.350855           0.147481         0.223882   \n",
       "2    0.144564          0.364723           0.125263         0.096326   \n",
       "3    0.096908          0.149970           0.060628         0.041422   \n",
       "4    0.130333          0.379912           0.120315         0.049768   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0              0.820619        0.237138                 0.138463  \n",
       "1              0.377663        0.007491                 0.086187  \n",
       "2              0.299107        0.244628                 0.149416  \n",
       "3              0.164021        0.121033                 0.089663  \n",
       "4              0.273643        0.130298                 0.138594  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the first five rows of diagnosis training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis\n",
       "0          1\n",
       "1          1\n",
       "2          0\n",
       "3          0\n",
       "4          0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define all of the constant parameters that we will need in order to run the gradient descent like the learning rate $\\alpha$, the number of iterations, as well as the number of hidden layers and the hidden inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = trainY.values\n",
    "trainY = trainY.reshape((len(trainY),1))\n",
    "\n",
    "testY = testY.values\n",
    "testY = testY.reshape((len(testY),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerDimension = [30, 5, 3, 1] #4 layers with 2 hidden layers\n",
    "learningRate = 0.0075\n",
    "numIterations = 200010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the classification that will be conducted is about the severity of the tumor: whether the tumor is benign ot malignant, then the accuracy metrics would not be the best metrics for this condition. The emphasize should be focused in supressing the amount of false negative in the algorithm because the cost of having miss-classified an actual positive will be huge. \n",
    "\n",
    "Just imagine where there is a patient that has a breast tumor and we make a decision that the tumor is benign when in fact it is malignant. This situation will endanger the patient life and thus, creating a machine learning algorithm that can supress the amount of false negative should be a priority.\n",
    "\n",
    "Because of this, instead of accuracy or Jaccard index, the metrics that will be used for this problem is F1-score, which is the 'average' of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, activation):\n",
    "    \n",
    "    m = X.shape[1] \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = modelForwardProp(X, parameters, activation)\n",
    "\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    truePositive = []\n",
    "    trueNegative = []\n",
    "    falsePositive = []\n",
    "    falseNegative = []\n",
    "    \n",
    "    for i in range (0, probas.shape[1]):\n",
    "        truePositive.append((p[0,i] == 1 and y[0,i] == 1))\n",
    "        trueNegative.append(p[0,i] == 0 and y[0,i] == 0)\n",
    "        falsePositive.append(p[0,i] == 1 and y[0,i] == 0)\n",
    "        falseNegative.append(p[0,i] == 0 and y[0,i] == 1)\n",
    "    \n",
    "    epsilon = 10e-8\n",
    "    recall = truePositive.count(True)/(truePositive.count(True)+falseNegative.count(True)+epsilon)\n",
    "    precision = truePositive.count(True)/(truePositive.count(True)+falsePositive.count(True)+epsilon)\n",
    "    \n",
    "    F1_score = 2*(precision*recall/(precision+recall+epsilon))\n",
    "        \n",
    "    return F1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a function that will run the whole gradient descent optimization depending the constant parameters that we defined already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentNN(X, Y, layerDimension, learningRate, numIterations, activation):\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                  \n",
    "\n",
    "    parameters = initializeParameters(layerDimension)\n",
    " \n",
    "    \n",
    "    # Loop gradient descent\n",
    "    for i in range(0, numIterations):\n",
    "\n",
    "        AL, caches = modelForwardProp(X, parameters, activation)\n",
    "  \n",
    "        cost = computeCost(AL, Y)\n",
    "        \n",
    "        grads = modelBackProp(AL, Y, caches, activation)\n",
    " \n",
    "        parameters = updateParameters(parameters, grads, learningRate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if i % 10000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if i % 10000 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLu Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the steps necessary for the gradient descent to work, next we can predict the F1 score of the deep neural networks using ReLu or rectified linear unit activation function. To do this, we just need to pass 'relu' argument whenever we call the gradient descent function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 10000: 0.653012\n",
      "Cost after iteration 20000: 0.653012\n",
      "Cost after iteration 30000: 0.653012\n",
      "Cost after iteration 40000: 0.653012\n",
      "Cost after iteration 50000: 0.653012\n",
      "Cost after iteration 60000: 0.653012\n",
      "Cost after iteration 70000: 0.653012\n",
      "Cost after iteration 80000: 0.653012\n",
      "Cost after iteration 90000: 0.653012\n",
      "Cost after iteration 100000: 0.653012\n",
      "Cost after iteration 110000: 0.653012\n",
      "Cost after iteration 120000: 0.653011\n",
      "Cost after iteration 130000: 0.653010\n",
      "Cost after iteration 140000: 0.653008\n",
      "Cost after iteration 150000: 0.653003\n",
      "Cost after iteration 160000: 0.652975\n",
      "Cost after iteration 170000: 0.652561\n",
      "Cost after iteration 180000: 0.158762\n",
      "Cost after iteration 190000: 0.056729\n",
      "Cost after iteration 200000: 0.045309\n"
     ]
    }
   ],
   "source": [
    "parameters = gradientDescentNN(trainX.T, trainY.T, layerDimension, learningRate, numIterations, 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can predict the F1 score for both training set and the test set based on the Neural Networks model that has just been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9858155521477819"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTrain = predict(trainX.T, trainY.T, parameters, 'relu')\n",
    "predictTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473683696986855"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTest = predict(testX.T, testY.T, parameters, 'relu')\n",
    "predictTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the neural networks with ReLu activation functions has the F1 score in the training set of 98,5%, which is impressive, but the performance on the test set is just 94.7%. Let's see the comparison with the neural networks with sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of observing neural networks' F1 score with sigmoid activation function is the same with the one with relu, but now instead of 'relu', we pass 'sigmoid' as an argument whenever calling gradient descent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693042\n",
      "Cost after iteration 10000: 0.653010\n",
      "Cost after iteration 20000: 0.653008\n",
      "Cost after iteration 30000: 0.653005\n",
      "Cost after iteration 40000: 0.652999\n",
      "Cost after iteration 50000: 0.652987\n",
      "Cost after iteration 60000: 0.652966\n",
      "Cost after iteration 70000: 0.652925\n",
      "Cost after iteration 80000: 0.652841\n",
      "Cost after iteration 90000: 0.652654\n",
      "Cost after iteration 100000: 0.652176\n",
      "Cost after iteration 110000: 0.650662\n",
      "Cost after iteration 120000: 0.644215\n",
      "Cost after iteration 130000: 0.597069\n",
      "Cost after iteration 140000: 0.270989\n",
      "Cost after iteration 150000: 0.139028\n",
      "Cost after iteration 160000: 0.100634\n",
      "Cost after iteration 170000: 0.083415\n",
      "Cost after iteration 180000: 0.073823\n",
      "Cost after iteration 190000: 0.067507\n",
      "Cost after iteration 200000: 0.062954\n"
     ]
    }
   ],
   "source": [
    "parameters = gradientDescentNN(trainX.T, trainY.T, layerDimension, learningRate, numIterations, 'sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can predict the F1 score in the training set as well as in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787233535712514"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTrain = predict(trainX.T, trainY.T, parameters, 'sigmoid')\n",
    "predictTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624059636610349"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTest = predict(testX.T, testY.T, parameters, 'sigmoid')\n",
    "predictTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the F1 score for the training set of neural networks with sigmoid function is 97,8% which is worse than the one with ReLu activation functions. However, we can see that for the test set, the neural networks with sigmoid activation function has 96.2% of F1 score, which outperforms the one with ReLu function.\n",
    "\n",
    "Now, let's see how the neural networks with tanh activation function performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tanh Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the tanh function, the adjustment of learning rate parameter is necessary since if the learning rate is kept the same as the two activation functions before, then the gradient descent would work due to either numerical instability or because the gradient just blow up. In order to avoid this, we need to decrease the learning rate so that the learning algorithm takes a baby step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 10000: 0.653013\n",
      "Cost after iteration 20000: 0.653012\n",
      "Cost after iteration 30000: 0.653011\n",
      "Cost after iteration 40000: 0.653011\n",
      "Cost after iteration 50000: 0.653010\n",
      "Cost after iteration 60000: 0.653008\n",
      "Cost after iteration 70000: 0.653004\n",
      "Cost after iteration 80000: 0.652997\n",
      "Cost after iteration 90000: 0.652978\n",
      "Cost after iteration 100000: 0.652913\n",
      "Cost after iteration 110000: 0.652502\n",
      "Cost after iteration 120000: 0.638753\n",
      "Cost after iteration 130000: 0.190986\n",
      "Cost after iteration 140000: 0.104909\n",
      "Cost after iteration 150000: 0.081382\n",
      "Cost after iteration 160000: 0.069937\n",
      "Cost after iteration 170000: 0.062818\n",
      "Cost after iteration 180000: 0.057885\n",
      "Cost after iteration 190000: 0.054246\n",
      "Cost after iteration 200000: 0.051401\n"
     ]
    }
   ],
   "source": [
    "parameters = gradientDescentNN(trainX.T, trainY.T, layerDimension, learningRate, numIterations, 'tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can predict the F1 score in the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9823321047884256"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTrain = predict(trainX.T, trainY.T, parameters, 'tanh')\n",
    "predictTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624059636610349"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTest = predict(testX.T, testY.T, parameters, 'tanh')\n",
    "predictTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the tanh activation function has a F1 score of 98.2% in the training set, and 96.2% in test set, which is comparable with the one with sigmoid activation function. However, we can't conclude that this activation function performs better than the other because the hyperparameter, which is learning rate, is different than the other two activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural networks for breast cancer prediction with three different activation functions has been performed. Although all three activation functions give a very good F1 score, but perhaps it can be improved. In another project, this theme will be expanded and improved by applying different optimization algorithm other than gradient descent. The optimization algorithm that will be applied are Adam and momentum mini-batch gradient descent and then let's check whether there is improvements on the F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
