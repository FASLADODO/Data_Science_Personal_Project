{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Distancing Detection with Python, OpenCV, and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to build an algorithm to detect whether people are obeying the social distancing rule to avoid the further spread of Coronavirus. This project is just for fun purpose only and how we can utilize the feature of OpenCV and TensorFlow framework to create such a social distancing detection algorithm.\n",
    "\n",
    "In this project, the pre-trained TensorFlow model, which is Faster R-CNN ResNet trained on MS COCO 2017 dataset will be applied to predict the coordinate of the bounding boxes as well as to predict which object that each bounding box represents.\n",
    "\n",
    "Before we start to cerate the algorithm, let's import the necessary library for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import yaml\n",
    "import imutils\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import itertools\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model, Video, and Additional Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we should do is to assign the color of out bounding boxes. Now, there are going to be two colors that will be applied in this project: Green and Red. If the distance between 2 or more people are greater than the specified minimum distance, then the bounding box will have green color, otherwise it's going to be red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED = (0, 0, 255)\n",
    "GREEN = (0, 255, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the pre-trained model such that the model is ready to create a prediction. From the Faster R-CNN model trained on MS COCO dataset, we get the frozen inference graph of the model. This is a frozen graph which can't be trained anymore and it contains the graph definition of the model as well as trained parameters. \n",
    "\n",
    "To load the model, we define the class to instansiate the model and a function within the class to predict the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "   \n",
    "    def __init__(self, model_path):\n",
    "\n",
    "        \n",
    "        self.detection_graph = tf.Graph()\n",
    "        \n",
    "        # Load the model into the tensorflow graph\n",
    "        with self.detection_graph.as_default():\n",
    "            \n",
    "            od_graph_def = tf.compat.v1.GraphDef()\n",
    "            \n",
    "            with tf.io.gfile.GFile(model_path, 'rb') as file:\n",
    "                \n",
    "                serialized_graph = file.read()\n",
    "                print(serialized_graph)\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "        # Create a session from the detection graph\n",
    "        self.sess = tf.compat.v1.Session(graph=self.detection_graph)\n",
    "\n",
    "    def predict(self,img):\n",
    "        \n",
    "        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "        img_exp = np.expand_dims(img, axis=0)\n",
    "        \n",
    "        # Pass the inputs and outputs to the session to get the results \n",
    "        (boxes, scores, classes) = self.sess.run([self.detection_graph.get_tensor_by_name('detection_boxes:0'), self.detection_graph.get_tensor_by_name('detection_scores:0'), self.detection_graph.get_tensor_by_name('detection_classes:0')],feed_dict={self.detection_graph.get_tensor_by_name('image_tensor:0'): img_exp})\n",
    "        \n",
    "        return (boxes, scores, classes)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a yaml file consits of the configuration file. What's going to be in this configuration file is the corner points of each rectangle that will define the area in the image where our predictions are gonna take place. If the people are outside of this rectangle in the video, then we're not going to draw prediction or bounding boxes on them.\n",
    "\n",
    "Plus the configuration file is important to extract an example of the top view of an image such that at the end we can create a transformation matrix to change the perspective of the image from warped to top view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Loading config file for the bird view transformation ] \n",
      " Done : [ Config file loaded ] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(\"[ Loading config file for the bird view transformation ] \")\n",
    "\n",
    "with open(\"config.yml\", \"r\") as ymlfile:\n",
    "    cfg = yaml.load(ymlfile)\n",
    "width_og, height_og = 0,0\n",
    "corner_points = []\n",
    "\n",
    "for section in cfg:\n",
    "    corner_points.append(cfg[\"image_parameters\"][\"p1\"])\n",
    "    corner_points.append(cfg[\"image_parameters\"][\"p2\"])\n",
    "    corner_points.append(cfg[\"image_parameters\"][\"p3\"])\n",
    "    corner_points.append(cfg[\"image_parameters\"][\"p4\"])\n",
    "    \n",
    "    width_og = int(cfg[\"image_parameters\"][\"width_og\"])\n",
    "    height_og = int(cfg[\"image_parameters\"][\"height_og\"])\n",
    "    \n",
    "    img_path = cfg[\"image_parameters\"][\"img_path\"]\n",
    "   \n",
    "    \n",
    "print(\" Done : [ Config file loaded ] ...\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, what we need to do is to instansiate our model by specifying the path to the frozen graph of trained Faster R-CNN model in our directory, then pass the path as the argument to call the class that we have defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - faster_rcnn_resnet101_coco_11_06_2017 [0]\n",
      " - image [1]\n",
      " - video [2]\n",
      " [ Loading TensorFlow Model ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done : [ Model loaded and initialized ] ...\n"
     ]
    }
   ],
   "source": [
    "model_names_list = [name for name in os.listdir(\"C:/Users/ASUS/models/.\") if name.find(\".\") == -1]\n",
    "for index,model_name in enumerate(model_names_list):\n",
    "    print(\" - {} [{}]\".format(model_name,index))\n",
    "\n",
    "model_path=\"C:/Users/ASUS/models/faster_rcnn_resnet101_coco_11_06_2017/frozen_inference_graph.pb\" \n",
    "\n",
    "print( \" [ Loading TensorFlow Model ... ]\")\n",
    "model = Model(model_path)\n",
    "print(\"Done : [ Model loaded and initialized ] ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded our model, the next this is to specify which video that we want the model to predict. For the video to test the model, we are going to use [PETS 2009 dataset S2 for people tracking](http://www.cvg.reading.ac.uk/PETS2009/a.html#s0). If you have other similar video, feel free to change it to yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - bird_view.avi [0]\n",
      " - bird_view_video.avi [1]\n",
      " - output.avi [2]\n",
      " - output_video.avi [3]\n",
      " - PETS2009.avi [4]\n"
     ]
    }
   ],
   "source": [
    "video_names_list = [name for name in os.listdir(\"C:/Users/ASUS/models/video/.\") if name.endswith(\".mp4\") or name.endswith(\".avi\")]\n",
    "for index,video_name in enumerate(video_names_list):\n",
    "    print(\" - {} [{}]\".format(video_name,index))\n",
    "\n",
    "video_path=\"C:/Users/ASUS/models/video/PETS2009.avi\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to specify how close each person to another person should be such that we are deemed that they are too close, hence we turn their bounding box from green to red. You can change the value to any value that you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_minimum = \"110\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Transformation Matrix for Perspective Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we done with loading the model, specifying the video that we want to use, the next this is to define a transformation matrix using OpenCV library. But why do we need a transformation matrix?\n",
    "\n",
    "The idea of this transformation matrix is to change the warped perspective of the camera to bird's eye view (or top view). We need to map the perspective into top view because then it will be easier for us to compute the distance between two or more people. It's going to be difficult to compute the distance between two coordinates in a warped perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perspective_transform(corner_points,width,height,image):\n",
    "\n",
    "    corner_points_array = np.float32(corner_points)\n",
    "    # Create an array with the parameters (the dimensions) required to build the matrix\n",
    "    img_params = np.float32([[0,0],[width,0],[0,height],[width,height]])\n",
    "    \n",
    "    matrix = cv2.getPerspectiveTransform(corner_points_array,img_params) \n",
    "    img_transformed = cv2.warpPerspective(image,matrix,(width,height))\n",
    "    \n",
    "    return matrix,img_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined a function to compute the transformation matrix above, next what we should do is to use the resulting transformation matrix to transform each centroid of bounding boxes from warped perspective into top view perspective. The result of the function below is the coordinate of each centroid of bounding boxes in a bird's eye view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_point_perspective_transformation(matrix,list_centroids):\n",
    "\n",
    " \n",
    "    list_points_to_detect = np.float32(list_centroids).reshape(-1, 1, 2)\n",
    "    transformed_points = cv2.perspectiveTransform(list_points_to_detect, matrix)\n",
    "    \n",
    "    # Loop over the points and add them to the list that will be returned\n",
    "    transformed_points_list = list()\n",
    "    \n",
    "    for i in range(0,transformed_points.shape[0]):\n",
    "        transformed_points_list.append([transformed_points[i][0][0],transformed_points[i][0][1]])\n",
    "        \n",
    "    return transformed_points_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the transformation matrix function, next we can finally use the function to transform the corner points in the warped perspective of the example image into bird's eye view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix,imgOutput = compute_perspective_transform(corner_points,width_og,height_og,cv2.imread(img_path))\n",
    "height,width,_ = imgOutput.shape\n",
    "blank_image = np.zeros((height,width,3), np.uint8)\n",
    "height = blank_image.shape[0]\n",
    "width = blank_image.shape[1] \n",
    "dim = (width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Bounding Boxes and Compute the Centroid of Each Bounding Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we done with defining the transformation matrix, the next thing that we need to do is to define a function to return only the bounding box whih represents humans. With the pre-trained model, we will get a lot of bounding boxes and not all of those bounding boxes represents human. Hence in the following function, only bounding boxes that represent human will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_box_detection(boxes,scores,classes,height,width):\n",
    "\n",
    "    array_boxes = list() \n",
    "    \n",
    "    for i in range(boxes.shape[1]):\n",
    "        # If the class of the detected object is 1 and the confidence of the prediction is > 0.75\n",
    "        if int(classes[i]) == 1 and scores[i] > 0.75:\n",
    "            \n",
    "            box = [boxes[0,i,0],boxes[0,i,1],boxes[0,i,2],boxes[0,i,3]] * np.array([height, width, height, width])\n",
    "            \n",
    "            array_boxes.append((int(box[0]),int(box[1]),int(box[2]),int(box[3])))\n",
    "            \n",
    "    return array_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after we define a function to return only bounding boxes that only represent humans, then we need to compute the centroid of each bounding boxes. The coordinate of the centroid of bounding boxes will be very important to compute the distance between one person to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(array_boxes_detected):\n",
    "\n",
    "    array_centroids = list() # Initialize empty centroid and ground point lists \n",
    "    for index,box in enumerate(array_boxes_detected):\n",
    "    \n",
    "        center_x = int(((box[1]+box[3])/2))\n",
    "        center_y = int(((box[0]+box[2])/2))\n",
    "        \n",
    "        array_centroids.append((center_x, center_y))\n",
    "       \n",
    "    return array_centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a function to draw the rectangle from corner points that we have defined in the configuration yaml file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle(corner_points, frame):\n",
    "\n",
    "    cv2.line(frame, (corner_points[0][0], corner_points[0][1]), (corner_points[1][0], corner_points[1][1]), GREEN, thickness=1)\n",
    "    cv2.line(frame, (corner_points[1][0], corner_points[1][1]), (corner_points[3][0], corner_points[3][1]), GREEN, thickness=1)\n",
    "    cv2.line(frame, (corner_points[0][0], corner_points[0][1]), (corner_points[2][0], corner_points[2][1]), GREEN, thickness=1)\n",
    "    cv2.line(frame, (corner_points[3][0], corner_points[3][1]), (corner_points[2][0], corner_points[2][1]), GREEN, thickness=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Bounding Boxes and Social Distancing Result of the Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow of predicting bounding boxes and social distancing result of the video is as follows:\n",
    "    \n",
    " - Read the video frame by frame.\n",
    "    \n",
    "   For each frame:\n",
    "    - Predict the boxes, classes, and scores using the pre-trained model that has been defined above.\n",
    "    \n",
    "       At the end of prediction, we'll get:\n",
    "       - boxes: The amount of boxes predicted in a single frame.\n",
    "       - classes: which class each box represents. If class = 1, it means it represents human.\n",
    "       - score: the confidence level of the prediction in which the model predicts whether the box represents the corresponding class.\n",
    "       \n",
    "    - Check how many boxes are classified as a human based on each box classes and its corresponding confidence.\n",
    "    - Foe every human boxes, we need to compute the centroids of the boxes.\n",
    "    - Next, we need to transform the centroid into the mapped perspective with the transformation matrix.\n",
    "    - Next, check how many boxes or transformed centroids that we have, if it is 2 or more people, then proceed to the next step, otherwise we don't need to detect anything (bc we dont want to detect only 1 person)\n",
    "    - Next, using itertools, we can create the permutation of different combinations of boxes coordinates. If the L2 distance (? Needs to be checked) of any of those permutations below the defined minimum distance, then we should change the rectangle color between these pairs of boxes into red.\n",
    "    - Write every frame back into a video.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "vs = cv2.VideoCapture(video_path)\n",
    "output_video_1 = None\n",
    "# Loop until the end of the video stream\n",
    "while True:\n",
    "    # Load the image of the ground and resize it to the correct size\n",
    "    img = cv2.imread(\"C:/Users/ASUS/models/image/chemin_1.png\")\n",
    "    bird_view_img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    # Load the frame\n",
    "    (frame_exists, frame) = vs.read()\n",
    "    \n",
    "    print(frame_exists)\n",
    "   \n",
    "    if not frame_exists:\n",
    "        break\n",
    "    else:\n",
    "\n",
    "        # Make the predictions for a frame\n",
    "        (boxes, scores, classes) =  model.predict(frame)\n",
    "       \n",
    "\n",
    "        # Return only boundix boxes that represent humans \n",
    "        array_boxes_detected = get_human_box_detection(boxes,scores[0].tolist(),classes[0].tolist(),frame.shape[0],frame.shape[1])\n",
    "\n",
    "        # Compute the centroids of each bounding boxes\n",
    "        array_centroids= get_centroids(array_boxes_detected)\n",
    "\n",
    "        # Use the transform matrix to get the transformed coordinates\n",
    "        transformed_centroids = compute_point_perspective_transformation(matrix, array_centroids)\n",
    "\n",
    "        # Check if 2 or more people have been detected (otherwise no need to detect)\n",
    "        if len(transformed_centroids) >= 2:\n",
    "            for index, centroid in enumerate(transformed_centroids):\n",
    "                if not (centroid[0] > width or centroid[0] < 0 or centroid[1] > height+200 or centroid[1] < 0 ):\n",
    "                    cv2.rectangle(frame,(array_boxes_detected[index][1],array_boxes_detected[index][0]),(array_boxes_detected[index][3],array_boxes_detected[index][2]),GREEN,2)\n",
    "\n",
    "            # Iterate over every possible permutations of the transformed centroids\n",
    "            list_indexes = list(itertools.combinations(range(len(transformed_centroids)), 2))\n",
    "            \n",
    "            for i,pair in enumerate(itertools.combinations(transformed_centroids, r=2)):\n",
    "                \n",
    "                # Check if the distance between each combination of points is less than the minimum distance\n",
    "                if math.sqrt( (pair[0][0] - pair[1][0])**2 + (pair[0][1] - pair[1][1])**2 ) < int(distance_minimum):\n",
    "                    \n",
    "                    # Change the colors of the points that are too close from each other to red\n",
    "                    if not (pair[0][0] > width or pair[0][0] < 0 or pair[0][1] > height+200  or pair[0][1] < 0 or pair[1][0] > width or pair[1][0] < 0 or pair[1][1] > height+200  or pair[1][1] < 0):\n",
    "                       \n",
    "                        index_pt1 = list_indexes[i][0]\n",
    "                        index_pt2 = list_indexes[i][1]\n",
    "                        \n",
    "                        cv2.rectangle(frame,(array_boxes_detected[index_pt1][1],array_boxes_detected[index_pt1][0]),(array_boxes_detected[index_pt1][3],array_boxes_detected[index_pt1][2]),RED,2)\n",
    "                        cv2.rectangle(frame,(array_boxes_detected[index_pt2][1],array_boxes_detected[index_pt2][0]),(array_boxes_detected[index_pt2][3],array_boxes_detected[index_pt2][2]),RED,2)\n",
    "\n",
    "\n",
    "    # Draw the rectangle of the area in the images where the detection is considered\n",
    "    draw_rectangle(corner_points, frame)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # Write the both outputs video to a local folders\n",
    "    if output_video_1 is None:\n",
    "        fourcc1 = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        output_video_1 = cv2.VideoWriter(\"C:/Users/ASUS/models/video/output_video_i.avi\", fourcc1, 25,(frame.shape[1], frame.shape[0]), True)\n",
    "       \n",
    "    elif output_video_1 is not None:\n",
    "        output_video_1.write(frame)\n",
    "\n",
    "    # Break the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
