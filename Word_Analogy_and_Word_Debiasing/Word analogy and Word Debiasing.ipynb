{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Analogy and Word Debiasing Using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to create a word analogy with the help of pre-trained word embeddings. By the end of this project, we can create an algorithm such that if we input large -> larger, then the computer can perceive small -> smaller.\n",
    "\n",
    "In addition to that, word debiasing algorithm will also be implemented because some words should always be in a neutral space in a finite word-embedding dimensions, i.e should not be biased towards positive or negative section in word-embedding dimensions.\n",
    "\n",
    "We only need numpy library for this project, so let's import numpy library first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a function to read the pre-trained word embeddings. The pre-trained model of word embeddings used in this project can be seen at https://nlp.stanford.edu/projects/glove/. This global vectors of words represents 100-dimension of embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    \n",
    "    with open(glove_file,'r', encoding=\"utf8\", ) as f:\n",
    "        \n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        print(f)\n",
    "        for line in f:\n",
    "            \n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='glove.6B.100d.txt' mode='r' encoding='utf8'>\n"
     ]
    }
   ],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, `words` parameter represents set of words contained in the global vectors, while `word-to_vec_map` represents the dictionary mapping of set of words to their global vectors representation.\n",
    "\n",
    "After reading the pre-trained global vectors, now we can start to build word analogy algorithm. As we know already, in order to create a word analogy, we need to compute the distance between words in high-dimensional embedding space. The distance between words can be a good measure how similar or disimilar each word is to the others.\n",
    "\n",
    "In order to compute the distance, cosine similarity algorithm will be used. Below is the mathematical equation on how to investigate the similarity using cosine similarity:\n",
    "\n",
    "$$\\text{CosSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)$$\n",
    "\n",
    "$$ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$$\n",
    "\n",
    "In above equation, $u.v$ is the dot products between word vectors $u$ and $v$, $||u||_2$ is the norm (or length) of the vector $u$, and $\\theta$ is the angle between $u$ and $v$. \n",
    "\n",
    "If the $CosSimilarity(u,v)$ is close to 1, it means that the two words are similar, while if they are dissimilar, then the value will be lower.\n",
    "\n",
    "Let's define a function to compute cosine similarity based on the equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity (vector_u, vector_v):\n",
    "    \n",
    "    dot_product = np.dot(vector_u, vector_v)\n",
    "    \n",
    "    norm_u = np.sqrt(np.sum(vector_u**2))\n",
    "    norm_v = np.sqrt(np.sum(vector_v**2))\n",
    "    \n",
    "    cosine_similarity = dot_product/(norm_u*norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get better intuition about what the result of this cosine similarity represents, let's take a look at the image below.\n",
    "\n",
    "<img src=\"image_vec.png\" width=\"700\" height=\"200\">\n",
    "\n",
    "From image above, similar word vectors can be achieved for example if we have words \"german\" and \"spanish\" because both words represent nationality of a person. If we have words for example \"tiger\" and \"stadium\" then probably we will get a dissimilar vector. However, if we have words such as \"Indonesia-Jakarta\" and \"Berlin-Germany\", then we get a similar vectors but they are pointing to the opposite directions.  \n",
    "\n",
    "Let's prove this theory with some validations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(german, spanish) =  0.60572438482194\n",
      "cosine_similarity(tiger, stadium) =  0.24485984266758953\n",
      "cosine_similarity(indonesia - jakarta, berlin - germany) =  -0.6505975768945929\n"
     ]
    }
   ],
   "source": [
    "german = word_to_vec_map[\"german\"]\n",
    "spanish = word_to_vec_map[\"spanish\"]\n",
    "tiger = word_to_vec_map[\"tiger\"]\n",
    "stadium = word_to_vec_map[\"stadium\"]\n",
    "indonesia = word_to_vec_map[\"indonesia\"]\n",
    "jakarta = word_to_vec_map[\"jakarta\"]\n",
    "berlin = word_to_vec_map[\"berlin\"]\n",
    "germany = word_to_vec_map[\"germany\"]\n",
    "\n",
    "print(\"cosine_similarity(german, spanish) = \", cosine_similarity(german, spanish))\n",
    "print(\"cosine_similarity(tiger, stadium) = \",cosine_similarity(tiger, stadium))\n",
    "print(\"cosine_similarity(indonesia - jakarta, berlin - germany) = \",cosine_similarity(indonesia - jakarta, berlin - germany))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the result above, the word \"german\" and \"spanish\" has the highest cosine similarity between them because both of them represent the nationality of a person. Meanwhile the word \"tiger\" and \"stadium\" have a low cosine similarity between them because they are not correlated by any chance. Finally, the word \"Indonesia - Jakarta\" and \"Berlin - Germany\" are similar with each other but the vectors are pointing in the opposite directions since one of the word represent a country and its capital, while the other represent a country's capital and its country name.\n",
    "\n",
    "With this cosine algorithm, we can finaly create our own word analogy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cosine similarity algorithm, now we can build our own word analogy. With word analogy, the computer can find out about the related words with respect to our input words. Let's say we give the computer an example that \"small\" -> \"smaller\". If we input a word \"large\", then the computer will hopefully predict that the appropriate word output should be \"larger\". Let's define a function to create word analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(word_1, word_2, word_3, word_to_vec_map):\n",
    "    \n",
    "    word_1, word_2, word_3 = word_1.lower(), word_2.lower(), word_3.lower()\n",
    "    \n",
    "    vector_a, vector_b, vector_c = [word_to_vec_map.get(x) for x in [word_1, word_2, word_3]]\n",
    "    \n",
    "    bag_of_words = word_to_vec_map.keys()\n",
    "    max_cos_similarity = -1000\n",
    "    analogous_word = None\n",
    "    \n",
    "    input_words = set([word_1, word_2, word_3])\n",
    "    \n",
    "    for i in bag_of_words:\n",
    "        \n",
    "        if i in input_words:\n",
    "            continue\n",
    "        \n",
    "        cos_similarity = cosine_similarity((vector_b-vector_a), (word_to_vec_map[i]-vector_c))\n",
    "        \n",
    "        if cos_similarity > max_cos_similarity:\n",
    "            max_cos_similarity = cos_similarity\n",
    "            analogous_word = i\n",
    "    \n",
    "    return analogous_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's do the fun part. After applying the function above, we can now give a logical pair of words to the computer, and then let the computer guess the logical word of our input word. Let's say we give an example of paired logical words \"italy\" -> \"italian\" to the computer. Then if we give a new example such as \"france\", hopefully the computer will predict that the output is \"french\" based on the example that we gave to the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: france -> french\n",
      "paris -> french :: jakarta -> indonesian\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: big -> bigger\n"
     ]
    }
   ],
   "source": [
    "trials_to_try = [('italy', 'italian', 'france'), ('paris', 'french', 'jakarta'), ('man', 'woman', 'boy'), ('small', 'smaller', 'big')]\n",
    "for triad in trials_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, word_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks pretty good! From several examples above, it can be seen that the computer predicted the output words correctly in all of the paired logical words that we gave as examples. From the result above, we can see that cosine similarity is a good algorithm in order to find similarities of word vectors in a high dimensional embedding space.\n",
    "\n",
    "Next, let's talk about word debiasing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Debiasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because all of the words have been mapped into their corresponding embedding in high dimensional spaces, there is no doubt that there might be a potential of high bias among words. While this bias is not a problem for certain words, but for certain words this might be problematic. Let's see what this means in this word debiasing section.\n",
    "\n",
    "Suppose we want to empashize the word embedding that encodes the concept of gender, like man or woman, boy or girl, and father or mother. We then measure the similarity between these words and as the result, we get the vetor representation of gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_gender_1 = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "vec_gender_2 = word_to_vec_map['mother'] - word_to_vec_map['father']\n",
    "vec_gender_3 = word_to_vec_map['girl'] - word_to_vec_map['boy']\n",
    "\n",
    "vec_gender = (vec_gender_1 + vec_gender_2 + vec_gender_3)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of names and their similarities with constructed vector:\n",
      "cristiano -0.26978141682617657\n",
      "marie 0.27827313109500895\n",
      "sophie 0.29482186776178243\n",
      "lionel -0.1978422637542166\n",
      "liana 0.11854306625152639\n",
      "frank -0.2666248280507678\n",
      "danielle 0.2157615895497447\n",
      "ruben -0.1846352672830835\n",
      "katy 0.24279711205080473\n",
      "gwenn 0.06087660665040621\n"
     ]
    }
   ],
   "source": [
    "print ('List of names and their similarities with constructed vector:')\n",
    "\n",
    "# girls and boys name\n",
    "name_list = ['cristiano', 'marie', 'sophie', 'lionel', 'liana', 'frank', 'danielle', 'ruben', 'katy', 'gwenn']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], vec_gender))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above, we get either a value below 0 (negative value) or above 0 (positive value). The positive value favors slightly towards the name which has an association to female names (marie, sophie, liana, danielle, katy, gwenn) while the negative value favors slightly towards the name which has an association to male names (cristiano, lionel, frank, ruben). In the case of people's name, it is not surprising that the vector representation of different names has a bias towards people's gender.\n",
    "\n",
    "However, let's take a look at different kind of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other words and their similarities:\n",
      "lipstick 0.26239475958178377\n",
      "guns -0.026486816826763952\n",
      "science -0.02950717932770746\n",
      "arts -0.009740465149808631\n",
      "literature 0.0335872135277116\n",
      "warrior -0.0924716650655859\n",
      "doctor 0.04784108101903545\n",
      "make-up 0.19464814971231043\n",
      "receptionist 0.2863207848022488\n",
      "technology -0.1471145519580031\n",
      "fashion 0.15605776370152577\n",
      "babysitter 0.2448051299671224\n",
      "engineer -0.24223725681660616\n",
      "pilot -0.07853385488822553\n",
      "computer -0.14053746067593784\n",
      "singer 0.10582590977203045\n"
     ]
    }
   ],
   "source": [
    "print('Other words and their similarities:')\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'make-up', 'receptionist', \n",
    "             'technology',  'fashion', 'babysitter', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], vec_gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the result above, it turns out that all of the thing that seems unrelated to gender has some biases in them. For example, the word \"engineer\" has negative value, which means that it leans heavier towards \"man\", while \"babysitter\" has positive value, which means that it leans heavier toward \"woman\". This phenomenon is unacceptable since these words should be neutral and should not reflect any unhealthy gender stereotype.\n",
    "\n",
    "In order to fix this phenomenon, we need to do some word debiasing such that the words that has no relation whatsoever with gender can be neutralized. Below is the mathematical formulation to neutralize bias in certain word embeddings:\n",
    "\n",
    "\n",
    "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g$$\n",
    "$$e^{debiased} = e - e^{bias\\_component}$$\n",
    "\n",
    "where $e$ is the vector embedding of certain word in a finite dimensional space and $g$ is the bias direction. The formula above takes vector embedding of words and then zeros out the bias direction, creating a new debiased vector embedding representing the same words.\n",
    "\n",
    "Let's implement formula above in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_debiasing(word, vec_gender, word_to_vec_map):\n",
    "    \n",
    "    vec_word = word_to_vec_map[word]\n",
    "    \n",
    "    vec_word_biased = np.dot(vec_word, vec_gender)*vec_gender/(np.sum(vec_gender**2))\n",
    "    \n",
    "    vec_word_debiased = vec_word - vec_word_biased\n",
    "    \n",
    "    return vec_word_debiased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can call the function that we defined above and let's investigate the vector embedding of the word \"babysitter\" before the application of debiasing and after the application of word debiasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between babysitter and g, before neutralizing:  0.2448051299671224\n",
      "cosine similarity between babysitter and g, after neutralizing:  -5.956249596946488e-18\n"
     ]
    }
   ],
   "source": [
    "e = \"babysitter\"\n",
    "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"babysitter\"], vec_gender))\n",
    "\n",
    "e_debiased = word_debiasing(\"babysitter\", vec_gender, word_to_vec_map)\n",
    "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, vec_gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output, before the application of embedding debiasing, the cosine similarity between the word \"babysitter\" and gender bias direction $g$ is 0.244, which means that this word leans heavier towards the word \"woman\", \"girl\", etc. However, after the application of word debiasing, now this word has the cosine similarity of basically 0, which means that this word is no longer associated with a bias towards one gender, which is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
